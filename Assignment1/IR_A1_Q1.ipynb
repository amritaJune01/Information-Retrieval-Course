{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3aaeb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13bcc14",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print file contents\n",
    "def printFileContent(path1, path2):\n",
    "    for file in os.listdir(path1)[:5]:\n",
    "        f1 = open(f'{path1}/{file}', 'r')\n",
    "        f2 = open(f\"{path2}/{file}\", 'r')\n",
    "        print(f\"\\nContent of file {file} before text extraction\\n\")\n",
    "        print(f1.read())    \n",
    "        print(f\"\\nContent of file {file} after text extraction\\n\")\n",
    "        print(f2.read())\n",
    "        print(\"\\n-----------------------------------------------------------\\n\")\n",
    "        f1.close()\n",
    "        f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fe12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text between tags from all files\n",
    "def extractText(path):\n",
    "    for file in os.listdir(path):\n",
    "        f = open(f\"{path}/{file}\", 'r')\n",
    "        text = f.read() \n",
    "        f.close()\n",
    "        x = re.search(\"((?<=\\<TITLE\\>)(.|\\n)*(?=\\<\\/TITLE\\>))\", text)\n",
    "        y = re.search(\"((?<=\\<TEXT\\>)(.|\\n)*(?=\\<\\/TEXT\\>))\", text)\n",
    "\n",
    "        z = x.group(0).strip() + ' ' + y.group().strip()\n",
    "        z = \" \".join(z.split())\n",
    "        f = open(f\"{path}/{file}\", 'w')\n",
    "        f.write(z)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./Assignment1/CSE508_Winter2023_Dataset1/CSE508_Winter2023_Dataset\"\n",
    "path2 = \"./Assignment1/CSE508_Winter2023_Dataset/CSE508_Winter2023_Dataset\"\n",
    "# path1 = \"Desktop/IR/Assignments/CSE508_Winter2023_Dataset_1\"\n",
    "# path2 = \"Desktop/IR/Assignments/CSE508_Winter2023_Dataset_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1) i\n",
    "# extractText(path2)\n",
    "printFileContent(path1, path2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f51915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lowercase the text\n",
    "def toLowerCase(path,df):\n",
    "    for file in os.listdir(path):\n",
    "        f = open(f\"{path2}/{file}\", 'r')\n",
    "        text = f.read()\n",
    "        lower_text = text.lower()\n",
    "        df.loc[len(df.index)] = [file,text,lower_text]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2) ii) 1\n",
    "lowercase_df = pd.DataFrame(columns = ['Doc Id','Before Lowercase','After Lowercase'])\n",
    "toLowerCase(path2,lowercase_df)\n",
    "lowercase_df.set_index('Doc Id',inplace = True)\n",
    "lowercase_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "def tokenizer(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        f = open(f\"{path2}/{file}\", 'r')\n",
    "        text = f.read()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokenized_text = str(tokens)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Lowercase'],tokenized_text]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78689527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2) ii) 2\n",
    "tokenize_df = pd.DataFrame(columns = ['Doc Id','Before Tokenization','After Tokenization'])\n",
    "tokenizer(path2,tokenize_df,lowercase_df)\n",
    "tokenize_df.set_index('Doc Id',inplace = True)\n",
    "tokenize_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stop words\n",
    "def removeStopWords(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        tokens = ast.literal_eval(prev_df.at[file,'After Tokenization'])\n",
    "        remove_punc = [t for t in tokens if not t in stopwords.words(\"english\")]\n",
    "        res = str(remove_punc)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Tokenization'],res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2) ii) 3\n",
    "stopwords_df = pd.DataFrame(columns = ['Doc Id','Before Stopwords Removal','After Stopwords Removal'])\n",
    "removeStopWords(path2,stopwords_df,tokenize_df)\n",
    "stopwords_df.set_index('Doc Id',inplace = True)\n",
    "stopwords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cc9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove punctuation\n",
    "punctuations = string.punctuation\n",
    "punc = [ p for p in punctuations]\n",
    "\n",
    "def removePunctuation(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        tokens = ast.literal_eval(prev_df.at[file,'After Stopwords Removal'])\n",
    "        res = [t for t in tokens if t not in punc]\n",
    "        res = str(res)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Stopwords Removal'],res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2) ii) 4\n",
    "pucntuation_df = pd.DataFrame(columns = ['Doc Id','Before Punctuation Removal','After Punctuation Removal'])\n",
    "removePunctuation(path2,pucntuation_df,stopwords_df)\n",
    "pucntuation_df.set_index('Doc Id',inplace = True)\n",
    "pucntuation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove punctuation\n",
    "def removeBlanks(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        tokens = ast.literal_eval(prev_df.at[file,'After Punctuation Removal'])\n",
    "        res = [ t.strip() for t in tokens if len(t.strip()) != 0]\n",
    "        res = str(res)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Punctuation Removal'],res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2) ii) 5\n",
    "blanks_df = pd.DataFrame(columns = ['Doc Id','Before Blank Space Tokens Removal',\n",
    "                                    'After Blank Space Tokens Removal'])\n",
    "removeBlanks(path2,blanks_df,pucntuation_df)\n",
    "blanks_df.set_index('Doc Id',inplace = True)\n",
    "blanks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final pre-processed data\n",
    "final_df = blanks_df.drop('Before Blank Space Tokens Removal', axis=1)\n",
    "final_df.rename(columns = {'After Blank Space Tokens Removal':'Contents of file after Preprocessing'}, \n",
    "                inplace = True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81519d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"Desktop/final_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
